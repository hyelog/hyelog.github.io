---
layout: post
title: Study log of LLaMA
tags: [study, GPT, llama]
---

## July-27-23
1. Learn about LLaMA from Youtube
[LLaMA lecture](https://www.youtube.com/watch?v=jvYpv9VJBOA)
[LLaMA code](https://github.com/facebookresearch/llama/tree/main/llama)
LLaMA is only GPT3(tokenizer + model), 'only' the difference is that has RMS normalization. 

## July-28-23
### [Stanford Alphaca](https://www.youtube.com/watch?v=dLo4QkEq-Hg)

#### The code review about Self instruct

regen file : the file generated by the machine
scorer : checking the similarity between sentences
instructions : default 100
machine_instruction_data : if there are the data generated by the machine, you can use it.
all_instructions : test full data
    all_instructions_tokens : the token generated by the tokenizer
batch : total 5 batch size 
prompt_instructions : the prompt from seed tasks will be encoded

decoding_args : args for generating the GPT model

openai_completion : the library function by Open AI

results : 
- post processing from GPT3 response
    - Ex : The sentence with "Write a program" is removed

Processing with the number of instruction_data 

regen.json : Store the data generated by the machine

With 52000 sentences, generate the model during 3 hours.

* [stanford alphaca github](https://github.com/tatsu-lab/stanford_alpaca)
* [alpaca-lora](https://github.com/tloen/alpaca-lora)


## July-29-23
### [ChatLLaMA](https://www.youtube.com/watch?v=T1XadeiKl1M)
### [stanford alphaca fine-tuning](https://www.youtube.com/watch?v=u2tQYgrLouo)
